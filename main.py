#!/usr/bin/env python3
"""
V2EX å¤‡ä»½å·¥å…·
åŠŸèƒ½ï¼š
1. å¤‡ä»½æˆ‘çš„æ”¶è— (/my/topics)
2. å¤‡ä»½æˆ‘çš„å‘å¸– (/member/{username}/topics)
3. å¤‡ä»½æˆ‘çš„å›å¤ (/member/{username}/replies)
4. ä»é¦–é¡µè‡ªåŠ¨è·å–ç”¨æˆ·å
5. ä»æ–‡ä»¶è¯»å– Cookie (æ”¯æŒChromeå¯¼å‡ºæ ¼å¼)
6. æå–è¯¦ç»†ä¿¡æ¯ï¼ˆç‚¹èµæ•°ã€ç²¾ç¡®æ—¶é—´ç­‰ï¼‰
7. å»é‡åŠŸèƒ½
8. å¯¼å‡ºå¤šç§æ ¼å¼ï¼ˆJSONã€TXTã€Markdownï¼‰
"""

import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time
import re

# é…ç½®
BASE_URL = "https://v2ex.com"
COOKIE_FILE = "cookie.txt"
BACKUP_DIR = "backups"

def load_cookie(cookie_file=COOKIE_FILE):
    """ä»æ–‡ä»¶åŠ è½½ Cookieï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    try:
        with open(cookie_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                print(f"âœ— {cookie_file} æ–‡ä»¶ä¸ºç©º")
                return None
            
            # æ£€æµ‹ Cookie æ ¼å¼
            # æ ¼å¼1: Chrome å¯¼å‡ºçš„è¡¨æ ¼æ ¼å¼ (åˆ¶è¡¨ç¬¦åˆ†éš”)
            if '\t' in content and ('v2ex.com' in content or 'www.v2ex.com' in content):
                print("æ£€æµ‹åˆ° Chrome å¯¼å‡ºæ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...")
                cookies = {}
                lines = content.split('\n')
                
                for line in lines:
                    if not line.strip():
                        continue
                    
                    # åˆ†å‰²æ¯è¡Œ (æ ¼å¼: name \t value \t domain \t ...)
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        name = parts[0].strip()
                        value = parts[1].strip()
                        
                        # ç§»é™¤å€¼ä¸¤è¾¹çš„å¼•å·ï¼ˆå¦‚æœæœ‰ï¼‰
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        
                        # ä¿å­˜ cookie
                        if name and value:
                            cookies[name] = value
                
                if not cookies:
                    print(f"âœ— æœªèƒ½ä» Chrome æ ¼å¼ä¸­æå–æœ‰æ•ˆçš„ Cookie")
                    return None
                
                # è½¬æ¢ä¸º HTTP Cookie æ ¼å¼
                cookie_string = '; '.join([f"{k}={v}" for k, v in cookies.items()])
                print(f"âœ“ æˆåŠŸè½¬æ¢ {len(cookies)} ä¸ª Cookie æ¡ç›®")
                return cookie_string
            
            # æ ¼å¼2: æ ‡å‡† HTTP Cookie æ ¼å¼
            elif '=' in content:
                print("æ£€æµ‹åˆ°æ ‡å‡† Cookie æ ¼å¼")
                return content
            
            else:
                print(f"âœ— æ— æ³•è¯†åˆ«çš„ Cookie æ ¼å¼")
                return None
                
    except FileNotFoundError:
        print(f"âœ— æœªæ‰¾åˆ° {cookie_file} æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"âœ— è¯»å– Cookie æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def get_username_from_homepage(cookie):
    """ä»é¦–é¡µè·å–å½“å‰ç™»å½•çš„ç”¨æˆ·å"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Cookie": cookie,
    }
    
    try:
        response = requests.get(BASE_URL, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾ç”¨æˆ·åé“¾æ¥ (æ ¼å¼: <a href="/member/username" class="top">)
            user_link = soup.find('a', class_='top', href=re.compile(r'/member/'))
            if user_link:
                username = user_link.get('href').replace('/member/', '')
                print(f"âœ“ æ£€æµ‹åˆ°ç”¨æˆ·å: {username}")
                return username
            else:
                print("âœ— æ— æ³•ä»é¦–é¡µè·å–ç”¨æˆ·å")
                return None
        else:
            print(f"âœ— è·å–é¦–é¡µå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
            
    except Exception as e:
        print(f"âœ— è·å–ç”¨æˆ·åæ—¶å‡ºé”™: {e}")
        return None

def get_page(cookie, url):
    """è·å–é¡µé¢ HTML"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Cookie": cookie,
        "Referer": BASE_URL,
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            return response.text
        else:
            print(f"âœ— è·å–é¡µé¢å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"âœ— è¯·æ±‚å‡ºé”™: {e}")
        return None

def parse_topic_from_item(item):
    """ä»ä¸»é¢˜æ¡ç›®ä¸­è§£æä¿¡æ¯"""
    try:
        topic = {}
        
        # è·å–ä¸»é¢˜é“¾æ¥å’Œæ ‡é¢˜
        title_element = item.find('span', class_='item_title')
        if title_element:
            link = title_element.find('a')
            if link:
                topic['title'] = link.text.strip()
                topic['url'] = BASE_URL + link.get('href', '')
                # ä» URL ä¸­æå– topic_id
                match = re.search(r'/t/(\d+)', topic['url'])
                if match:
                    topic['id'] = match.group(1)
        
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        node_element = item.find('a', class_='node')
        if node_element:
            topic['node'] = node_element.text.strip()
            topic['node_url'] = BASE_URL + node_element.get('href', '')
        
        # è·å–ä½œè€…ä¿¡æ¯
        author_element = item.find('strong')
        if author_element:
            author_link = author_element.find('a')
            if author_link:
                topic['author'] = author_link.text.strip()
                topic['author_url'] = BASE_URL + author_link.get('href', '')
        
        # è·å–å›å¤æ•°
        count_element = item.find('a', class_='count_livid')
        if not count_element:
            count_element = item.find('a', class_='count_orange')
        if count_element:
            topic['replies'] = int(count_element.text.strip())
        else:
            topic['replies'] = 0
        
        # è·å–ç‚¹èµæ•°
        votes_element = item.find('div', class_='votes')
        if votes_element:
            votes_text = votes_element.get_text(strip=True)
            votes_match = re.search(r'(\d+)', votes_text)
            if votes_match:
                topic['votes'] = int(votes_match.group(1))
            else:
                topic['votes'] = 0
        else:
            topic['votes'] = 0
        
        # è·å–ç²¾ç¡®å‘å¸ƒæ—¶é—´
        topic_info = item.find('span', class_='topic_info')
        if topic_info:
            time_span = topic_info.find('span', title=True)
            if time_span:
                topic['created_time'] = time_span.get('title', '')
                topic['created_time_relative'] = time_span.get_text(strip=True)
            
            # è·å–æœ€åå›å¤è€…
            if 'æœ€åå›å¤æ¥è‡ª' in topic_info.get_text():
                last_reply_match = re.search(r'æœ€åå›å¤æ¥è‡ª.*?<strong><a[^>]*>([^<]+)</a>', str(topic_info))
                if last_reply_match:
                    topic['last_reply_user'] = last_reply_match.group(1)
        
        return topic
        
    except Exception as e:
        print(f"âœ— è§£æä¸»é¢˜æ—¶å‡ºé”™: {e}")
        return None

def parse_page(html, page_type, current_page_num):
    """
    è§£æé¡µé¢ï¼Œæå–æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯
    page_type: 'favorites' æˆ– 'topics'
    """
    soup = BeautifulSoup(html, 'html.parser')
    items = soup.find_all('div', class_='cell item')
    
    topics = []
    for item in items:
        topic = parse_topic_from_item(item)
        if topic:
            topics.append(topic)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
    has_next = False
    all_links = soup.find_all('a')
    page_numbers = set()
    
    # åŒ¹é…åˆ†é¡µé“¾æ¥ï¼šå¯èƒ½æ˜¯å®Œæ•´è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„
    # å®Œæ•´: /my/topics?p=2 æˆ– /member/user/topics?p=2
    # ç›¸å¯¹: ?p=2
    for link in all_links:
        href = link.get('href', '')
        if '?p=' in href or 'topics?p=' in href:
            try:
                # æå–é¡µç 
                page_num = int(href.split('p=')[1].split('&')[0].split('#')[0])
                if 1 <= page_num <= 1000:
                    page_numbers.add(page_num)
            except:
                pass
    
    if page_numbers and max(page_numbers) > current_page_num:
        has_next = True
    
    return topics, has_next

def remove_duplicates(topics):
    """æ ¹æ® topic ID å»é‡"""
    seen = set()
    unique_topics = []
    
    for topic in topics:
        topic_id = topic.get('id')
        if topic_id and topic_id not in seen:
            seen.add(topic_id)
            unique_topics.append(topic)
    
    return unique_topics

def save_topics(topics, filename_prefix, output_dir=BACKUP_DIR):
    """ä¿å­˜ä¸»é¢˜åˆ°æ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON æ ¼å¼
    json_filename = f"{output_dir}/{filename_prefix}_{timestamp}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(topics, f, indent=2, ensure_ascii=False)
    
    # TXT æ ¼å¼
    txt_filename = f"{output_dir}/{filename_prefix}_{timestamp}.txt"
    with open(txt_filename, 'w', encoding='utf-8') as f:
        f.write(f"V2EX å¤‡ä»½\n")
        f.write(f"å¤‡ä»½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è®¡: {len(topics)} ä¸ªä¸»é¢˜\n")
        f.write("=" * 60 + "\n\n")
        
        for i, topic in enumerate(topics, 1):
            f.write(f"{i}. {topic.get('title', 'N/A')}\n")
            f.write(f"   èŠ‚ç‚¹: {topic.get('node', 'N/A')} | ä½œè€…: {topic.get('author', 'N/A')}\n")
            f.write(f"   å›å¤: {topic.get('replies', 0)} | ç‚¹èµ: {topic.get('votes', 0)}\n")
            f.write(f"   é“¾æ¥: {topic.get('url', 'N/A')}\n")
            if topic.get('created_time'):
                f.write(f"   å‘å¸ƒ: {topic['created_time']}\n")
            f.write("\n")
    
    # Markdown æ ¼å¼
    md_filename = f"{output_dir}/{filename_prefix}_{timestamp}.md"
    with open(md_filename, 'w', encoding='utf-8') as f:
        f.write(f"# V2EX å¤‡ä»½\n\n")
        f.write(f"**å¤‡ä»½æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æ€»è®¡**: {len(topics)} ä¸ªä¸»é¢˜\n\n")
        f.write("## ğŸ“š æ‰€æœ‰ä¸»é¢˜\n\n")
        
        # æŒ‰èŠ‚ç‚¹åˆ†ç»„
        topics_by_node = {}
        for topic in topics:
            node = topic.get('node', 'æœªåˆ†ç±»')
            if node not in topics_by_node:
                topics_by_node[node] = []
            topics_by_node[node].append(topic)
        
        for node, node_topics in sorted(topics_by_node.items()):
            f.write(f"### {node} ({len(node_topics)})\n\n")
            for topic in node_topics:
                f.write(f"- **[{topic['title']}]({topic['url']})**\n")
                f.write(f"  - ä½œè€…: [{topic.get('author', 'N/A')}]({topic.get('author_url', '#')})\n")
                f.write(f"  - å›å¤: {topic.get('replies', 0)} | ç‚¹èµ: {topic.get('votes', 0)}\n")
                if topic.get('created_time'):
                    f.write(f"  - å‘å¸ƒæ—¶é—´: {topic['created_time']}\n")
                f.write("\n")
    
    return json_filename, txt_filename, md_filename

def backup_favorites(cookie, output_dir=BACKUP_DIR):
    """å¤‡ä»½æˆ‘çš„æ”¶è—"""
    print("\n" + "=" * 60)
    print("å¼€å§‹å¤‡ä»½: æˆ‘çš„æ”¶è—")
    print("=" * 60)
    
    all_topics = []
    page = 1
    max_pages = 1000
    
    while page <= max_pages:
        print(f"\næ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
        
        url = f"{BASE_URL}/my/topics"
        if page > 1:
            url += f"?p={page}"
        
        html = get_page(cookie, url)
        if not html:
            break
        
        # æ£€æŸ¥æ˜¯å¦ç™»å½•
        if 'ç™»å½•' in html and 'Google è´¦å·ç™»å½•' in html:
            print("\nâœ— Cookie å¯èƒ½å·²å¤±æ•ˆ!")
            return None
        
        topics, has_next = parse_page(html, 'favorites', page)
        
        if not topics:
            print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°å†…å®¹")
            break
        
        all_topics.extend(topics)
        print(f"âœ“ ç¬¬ {page} é¡µ: è·å–åˆ° {len(topics)} ä¸ªä¸»é¢˜ (ç´¯è®¡: {len(all_topics)})")
        
        # æ˜¾ç¤ºå‰3ä¸ª
        for i, topic in enumerate(topics[:3], 1):
            votes_info = f"ğŸ‘ {topic.get('votes', 0)}" if topic.get('votes', 0) > 0 else ""
            print(f"  {i}. {topic.get('title', 'N/A')} [{topic.get('replies', 0)} å›å¤] {votes_info}")
        
        if not has_next:
            print(f"\nâœ“ å·²åˆ°è¾¾æœ€åä¸€é¡µ (ç¬¬ {page} é¡µ)")
            break
        
        page += 1
        time.sleep(1)
    
    if all_topics:
        # å»é‡
        original_count = len(all_topics)
        all_topics = remove_duplicates(all_topics)
        if original_count > len(all_topics):
            print(f"\nâœ“ å»é‡: ç§»é™¤äº† {original_count - len(all_topics)} ä¸ªé‡å¤é¡¹")
        
        # ä¿å­˜
        json_file, txt_file, md_file = save_topics(all_topics, 'favorites', output_dir)
        
        print("\n" + "=" * 60)
        print("âœ“ æ”¶è—å¤‡ä»½å®Œæˆ!")
        print(f"  æ€»å…±æ”¶è—: {len(all_topics)} ä¸ªä¸»é¢˜")
        print(f"\næ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“„ JSON: {json_file}")
        print(f"  ğŸ“„ TXT:  {txt_file}")
        print(f"  ğŸ“„ MD:   {md_file}")
        print("=" * 60)
        
        return all_topics
    
    return None

def backup_user_topics(cookie, username, output_dir=BACKUP_DIR):
    """å¤‡ä»½æˆ‘çš„å‘å¸–"""
    print("\n" + "=" * 60)
    print(f"å¼€å§‹å¤‡ä»½: æˆ‘çš„å‘å¸– (ç”¨æˆ·: {username})")
    print("=" * 60)
    
    all_topics = []
    page = 1
    max_pages = 1000
    
    while page <= max_pages:
        print(f"\næ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
        
        url = f"{BASE_URL}/member/{username}/topics"
        if page > 1:
            url += f"?p={page}"
        
        html = get_page(cookie, url)
        if not html:
            break
        
        topics, has_next = parse_page(html, 'topics', page)
        
        if not topics:
            print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°å†…å®¹")
            break
        
        all_topics.extend(topics)
        print(f"âœ“ ç¬¬ {page} é¡µ: è·å–åˆ° {len(topics)} ä¸ªä¸»é¢˜ (ç´¯è®¡: {len(all_topics)})")
        
        # æ˜¾ç¤ºå‰3ä¸ª
        for i, topic in enumerate(topics[:3], 1):
            votes_info = f"ğŸ‘ {topic.get('votes', 0)}" if topic.get('votes', 0) > 0 else ""
            print(f"  {i}. {topic.get('title', 'N/A')} [{topic.get('replies', 0)} å›å¤] {votes_info}")
        
        if not has_next:
            print(f"\nâœ“ å·²åˆ°è¾¾æœ€åä¸€é¡µ (ç¬¬ {page} é¡µ)")
            break
        
        page += 1
        time.sleep(1)
    
    if all_topics:
        # å»é‡
        original_count = len(all_topics)
        all_topics = remove_duplicates(all_topics)
        if original_count > len(all_topics):
            print(f"\nâœ“ å»é‡: ç§»é™¤äº† {original_count - len(all_topics)} ä¸ªé‡å¤é¡¹")
        
        # ä¿å­˜
        json_file, txt_file, md_file = save_topics(all_topics, f'my_topics_{username}', output_dir)
        
        print("\n" + "=" * 60)
        print("âœ“ å‘å¸–å¤‡ä»½å®Œæˆ!")
        print(f"  æ€»å…±å‘å¸–: {len(all_topics)} ä¸ªä¸»é¢˜")
        print(f"\næ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“„ JSON: {json_file}")
        print(f"  ğŸ“„ TXT:  {txt_file}")
        print(f"  ğŸ“„ MD:   {md_file}")
        print("=" * 60)
        
        return all_topics
    
    return None

def parse_reply_item(dock_area, inner):
    """è§£æå•ä¸ªå›å¤æ¡ç›®"""
    try:
        reply = {}
        
        # ä» dock_area æå–ä¿¡æ¯
        dock_text = dock_area.get_text()
        
        # æå–æ—¶é—´
        time_span = dock_area.find('span', class_='fade')
        if time_span:
            reply['time'] = time_span.get_text(strip=True)
        
        # æå–ä¸»é¢˜ä¿¡æ¯ (å›å¤äº† XXX åˆ›å»ºçš„ä¸»é¢˜ â€º èŠ‚ç‚¹ â€º ä¸»é¢˜æ ‡é¢˜)
        links = dock_area.find_all('a')
        for i, link in enumerate(links):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # ä¸»é¢˜ä½œè€…
            if '/member/' in href and i == 0:
                reply['topic_author'] = text
            # èŠ‚ç‚¹
            elif '/go/' in href:
                reply['node'] = text
            # ä¸»é¢˜æ ‡é¢˜å’Œé“¾æ¥
            elif '/t/' in href:
                reply['topic_title'] = text
                reply['topic_url'] = BASE_URL + href if href.startswith('/') else href
                # æå– topic_id
                match = re.search(r'/t/(\d+)', href)
                if match:
                    reply['topic_id'] = match.group(1)
        
        # ä» inner æå–å›å¤å†…å®¹
        reply_content_div = inner.find('div', class_='reply_content')
        if reply_content_div:
            reply['content'] = reply_content_div.get_text(strip=True)
            # ä¿ç•™ HTML æ ¼å¼çš„å†…å®¹ï¼ˆç”¨äºå¯¼å‡ºï¼‰
            reply['content_html'] = str(reply_content_div)
        
        return reply
        
    except Exception as e:
        print(f"âœ— è§£æå›å¤æ—¶å‡ºé”™: {e}")
        return None

def backup_user_replies(cookie, username, output_dir=BACKUP_DIR):
    """å¤‡ä»½æˆ‘çš„å›å¤"""
    print("\n" + "=" * 60)
    print(f"å¼€å§‹å¤‡ä»½: æˆ‘çš„å›å¤ (ç”¨æˆ·: {username})")
    print("=" * 60)
    
    all_replies = []
    page = 1
    max_pages = 1000
    
    while page <= max_pages:
        print(f"\næ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
        
        url = f"{BASE_URL}/member/{username}/replies"
        if page > 1:
            url += f"?p={page}"
        
        html = get_page(cookie, url)
        if not html:
            break
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å›å¤ï¼ˆdock_area + inner é…å¯¹ï¼‰
        dock_areas = soup.find_all('div', class_='dock_area')
        
        if not dock_areas:
            print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°å›å¤")
            break
        
        page_replies = []
        for dock_area in dock_areas:
            # æ‰¾åˆ°å¯¹åº”çš„ inner æˆ– cell (æœ€åä¸€æ¡å¯èƒ½æ˜¯ cell)
            inner = dock_area.find_next_sibling('div', class_='inner')
            if not inner:
                # å°è¯•æŸ¥æ‰¾ cell (æŸäº›å›å¤ä½¿ç”¨ cell è€Œä¸æ˜¯ inner)
                inner = dock_area.find_next_sibling('div', class_='cell')
            
            if inner:
                reply = parse_reply_item(dock_area, inner)
                if reply:
                    page_replies.append(reply)
        
        if not page_replies:
            print(f"ç¬¬ {page} é¡µè§£æå¤±è´¥")
            break
        
        all_replies.extend(page_replies)
        print(f"âœ“ ç¬¬ {page} é¡µ: è·å–åˆ° {len(page_replies)} æ¡å›å¤ (ç´¯è®¡: {len(all_replies)})")
        
        # æ˜¾ç¤ºå‰å‡ æ¡
        for i, reply in enumerate(page_replies[:3], 1):
            topic_title = reply.get('topic_title', 'N/A')[:50]
            content_preview = reply.get('content', '')[:30]
            print(f"  {i}. {topic_title} - {content_preview}...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        has_next = False
        all_links = soup.find_all('a')
        page_numbers = set()
        
        for link in all_links:
            href = link.get('href', '')
            if '?p=' in href or 'replies?p=' in href:
                try:
                    page_num = int(href.split('p=')[1].split('&')[0].split('#')[0])
                    if 1 <= page_num <= 1000:
                        page_numbers.add(page_num)
                except:
                    pass
        
        if page_numbers and max(page_numbers) > page:
            has_next = True
        
        if not has_next:
            print(f"\nâœ“ å·²åˆ°è¾¾æœ€åä¸€é¡µ (ç¬¬ {page} é¡µ)")
            break
        
        page += 1
        time.sleep(1)
    
    if all_replies:
        # ä¿å­˜å›å¤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs(output_dir, exist_ok=True)
        
        # JSON æ ¼å¼
        json_file = os.path.join(output_dir, f'my_replies_{username}_{timestamp}.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(all_replies, f, ensure_ascii=False, indent=2)
        
        # TXT æ ¼å¼
        txt_file = os.path.join(output_dir, f'my_replies_{username}_{timestamp}.txt')
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"V2EX å›å¤å¤‡ä»½ - {username}\n")
            f.write(f"å¤‡ä»½æ—¶é—´: {datetime.now()}\n")
            f.write(f"æ€»å›å¤æ•°: {len(all_replies)}\n")
            f.write("=" * 80 + "\n\n")
            
            for i, reply in enumerate(all_replies, 1):
                f.write(f"{i}. {reply.get('time', 'N/A')}\n")
                f.write(f"   ä¸»é¢˜: {reply.get('topic_title', 'N/A')}\n")
                f.write(f"   ä½œè€…: {reply.get('topic_author', 'N/A')}\n")
                f.write(f"   èŠ‚ç‚¹: {reply.get('node', 'N/A')}\n")
                f.write(f"   é“¾æ¥: {reply.get('topic_url', 'N/A')}\n")
                f.write(f"   å›å¤å†…å®¹:\n")
                f.write(f"   {reply.get('content', 'N/A')}\n")
                f.write("\n" + "-" * 80 + "\n\n")
        
        # Markdown æ ¼å¼
        md_file = os.path.join(output_dir, f'my_replies_{username}_{timestamp}.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# V2EX å›å¤å¤‡ä»½ - {username}\n\n")
            f.write(f"**å¤‡ä»½æ—¶é—´**: {datetime.now()}\n\n")
            f.write(f"**æ€»å›å¤æ•°**: {len(all_replies)}\n\n")
            f.write("---\n\n")
            
            for i, reply in enumerate(all_replies, 1):
                f.write(f"## {i}. {reply.get('topic_title', 'N/A')}\n\n")
                f.write(f"- **æ—¶é—´**: {reply.get('time', 'N/A')}\n")
                f.write(f"- **ä¸»é¢˜ä½œè€…**: {reply.get('topic_author', 'N/A')}\n")
                f.write(f"- **èŠ‚ç‚¹**: {reply.get('node', 'N/A')}\n")
                f.write(f"- **é“¾æ¥**: [{reply.get('topic_url', 'N/A')}]({reply.get('topic_url', 'N/A')})\n\n")
                f.write(f"**å›å¤å†…å®¹**:\n\n")
                f.write(f"{reply.get('content', 'N/A')}\n\n")
                f.write("---\n\n")
        
        print("\n" + "=" * 60)
        print("âœ“ å›å¤å¤‡ä»½å®Œæˆ!")
        print(f"  æ€»å›å¤æ•°: {len(all_replies)} æ¡")
        print(f"\næ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“„ JSON: {json_file}")
        print(f"  ğŸ“„ TXT:  {txt_file}")
        print(f"  ğŸ“„ MD:   {md_file}")
        print("=" * 60)
        
        return all_replies
    
    return None

def test_cookie(cookie):
    """æµ‹è¯• Cookie æ˜¯å¦æœ‰æ•ˆ"""
    print("æ­£åœ¨æµ‹è¯• Cookie...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Cookie": cookie,
    }
    
    try:
        response = requests.get(f"{BASE_URL}/my/topics", headers=headers, timeout=10)
        
        if response.status_code == 200:
            if 'ç™»å½•' in response.text and 'Google è´¦å·ç™»å½•' in response.text:
                print("âœ— Cookie æ— æ•ˆæˆ–å·²è¿‡æœŸ")
                return False
            else:
                print("âœ“ Cookie éªŒè¯æˆåŠŸ!")
                return True
        else:
            print(f"âœ— è¯·æ±‚å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âœ— æµ‹è¯•å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("V2EX å¤‡ä»½å·¥å…·")
    print("åŠŸèƒ½: 1) å¤‡ä»½æˆ‘çš„æ”¶è—  2) å¤‡ä»½æˆ‘çš„å‘å¸–  3) å¤‡ä»½æˆ‘çš„å›å¤")
    print("=" * 60)
    
    # åŠ è½½ Cookie
    cookie = load_cookie()
    if not cookie:
        print("\nè·å– Cookie çš„æ­¥éª¤:")
        print("1. åœ¨æµè§ˆå™¨ä¸­ç™»å½• V2EX")
        print("2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("3. è¿›å…¥ åº”ç”¨ -> å­˜å‚¨ -> Cookies")
        print("4. å¤åˆ¶æ‰€æœ‰ Cookie å¹¶ä¿å­˜åˆ° cookie.txt")
        exit(1)
    
    # æµ‹è¯• Cookie
    if not test_cookie(cookie):
        print("\nè¯·æ£€æŸ¥ä½ çš„ Cookie æ˜¯å¦æ­£ç¡®")
        exit(1)
    
    # è·å–ç”¨æˆ·å
    username = get_username_from_homepage(cookie)
    if not username:
        print("\nâœ— æ— æ³•è·å–ç”¨æˆ·åï¼Œå°†åªå¤‡ä»½æ”¶è—")
    
    # 1. å¤‡ä»½æ”¶è—
    favorites = backup_favorites(cookie)
    
    # 2. å¤‡ä»½å‘å¸–
    if username:
        my_topics = backup_user_topics(cookie, username)
    
    # 3. å¤‡ä»½å›å¤
    if username:
        my_replies = backup_user_replies(cookie, username)
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰å¤‡ä»½ä»»åŠ¡å®Œæˆ!")
    print("=" * 60)
