#!/usr/bin/env python3
"""
V2EX æ”¶è—å¤‡ä»½å·¥å…· - å¢å¼ºç‰ˆ
æ–°å¢åŠŸèƒ½ï¼š
- ä»æ–‡ä»¶è¯»å– Cookie
- æå–æ›´å¤šä¿¡æ¯ï¼ˆç‚¹èµæ•°ã€ç²¾ç¡®æ—¶é—´ï¼‰
- å»é‡åŠŸèƒ½
- å¯¼å‡º Markdown æ ¼å¼
- å¤‡ä»½å¯¹æ¯”
- ç»Ÿè®¡åˆ†æ
"""

import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time
import re

# é…ç½®
BASE_URL = "https://v2ex.com"
COOKIE_FILE = "cookie.txt"
BACKUP_DIR = "backups"

def load_cookie(cookie_file=COOKIE_FILE):
    """ä»æ–‡ä»¶åŠ è½½ Cookieï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    try:
        with open(cookie_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                print(f"âœ— {cookie_file} æ–‡ä»¶ä¸ºç©º")
                return None
            
            # æ£€æµ‹ Cookie æ ¼å¼
            # æ ¼å¼1: Chrome å¯¼å‡ºçš„è¡¨æ ¼æ ¼å¼ (åˆ¶è¡¨ç¬¦åˆ†éš”)
            if '\t' in content and ('v2ex.com' in content or 'www.v2ex.com' in content):
                print("æ£€æµ‹åˆ° Chrome å¯¼å‡ºæ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...")
                cookies = {}
                lines = content.split('\n')
                
                for line in lines:
                    if not line.strip():
                        continue
                    
                    # åˆ†å‰²æ¯è¡Œ (æ ¼å¼: name \t value \t domain \t ...)
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        name = parts[0].strip()
                        value = parts[1].strip()
                        
                        # ç§»é™¤å€¼ä¸¤è¾¹çš„å¼•å·ï¼ˆå¦‚æœæœ‰ï¼‰
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        
                        # ä¿å­˜ cookie
                        if name and value:
                            cookies[name] = value
                
                if not cookies:
                    print(f"âœ— æœªèƒ½ä» Chrome æ ¼å¼ä¸­æå–æœ‰æ•ˆçš„ Cookie")
                    return None
                
                # è½¬æ¢ä¸º HTTP Cookie æ ¼å¼
                cookie_string = '; '.join([f"{k}={v}" for k, v in cookies.items()])
                print(f"âœ“ æˆåŠŸè½¬æ¢ {len(cookies)} ä¸ª Cookie æ¡ç›®")
                return cookie_string
            
            # æ ¼å¼2: æ ‡å‡† HTTP Cookie æ ¼å¼ (key=value; key=value)
            elif '=' in content:
                print("æ£€æµ‹åˆ°æ ‡å‡† Cookie æ ¼å¼")
                return content
            
            else:
                print(f"âœ— æ— æ³•è¯†åˆ«çš„ Cookie æ ¼å¼")
                print(f"æç¤º: æ”¯æŒçš„æ ¼å¼:")
                print(f"  1. Chrome å¯¼å‡ºçš„è¡¨æ ¼æ ¼å¼ (å¤åˆ¶ Cookie è¡¨æ ¼)")
                print(f"  2. æ ‡å‡† HTTP Cookie æ ¼å¼ (key=value; key=value)")
                return None
                
    except FileNotFoundError:
        print(f"âœ— æœªæ‰¾åˆ° {cookie_file} æ–‡ä»¶")
        print(f"æç¤º: è¯·åˆ›å»º {cookie_file} æ–‡ä»¶å¹¶å°†ä½ çš„ Cookie ç²˜è´´è¿›å»")
        return None
    except Exception as e:
        print(f"âœ— è¯»å– Cookie æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def get_favorites_page(cookie, page=1):
    """è·å–æ”¶è—é¡µé¢çš„ HTML"""
    url = f"{BASE_URL}/my/topics"
    if page > 1:
        url += f"?p={page}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Cookie": cookie,
        "Referer": BASE_URL,
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            return response.text
        else:
            print(f"âœ— è·å–ç¬¬ {page} é¡µå¤±è´¥, çŠ¶æ€ç : {response.status_code}")
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"âœ— è¯·æ±‚å‡ºé”™: {e}")
        return None

def parse_topic_from_item(item):
    """ä»æ”¶è—é¡¹ä¸­è§£æä¸»é¢˜ä¿¡æ¯"""
    try:
        topic = {}
        
        # è·å–ä¸»é¢˜é“¾æ¥å’Œæ ‡é¢˜
        title_element = item.find('span', class_='item_title')
        if title_element:
            link = title_element.find('a')
            if link:
                topic['title'] = link.text.strip()
                topic['url'] = BASE_URL + link.get('href', '')
                # ä» URL ä¸­æå– topic_id
                match = re.search(r'/t/(\d+)', topic['url'])
                if match:
                    topic['id'] = match.group(1)
        
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        node_element = item.find('a', class_='node')
        if node_element:
            topic['node'] = node_element.text.strip()
            topic['node_url'] = BASE_URL + node_element.get('href', '')
        
        # è·å–ä½œè€…ä¿¡æ¯
        author_element = item.find('strong')
        if author_element:
            author_link = author_element.find('a')
            if author_link:
                topic['author'] = author_link.text.strip()
                topic['author_url'] = BASE_URL + author_link.get('href', '')
        
        # è·å–ç”¨æˆ·å¤´åƒ
        avatar_element = item.find('img', class_='avatar')
        if avatar_element:
            topic['author_avatar'] = avatar_element.get('src', '')
        
        # è·å–å›å¤æ•°
        count_element = item.find('a', class_='count_livid')
        if not count_element:
            count_element = item.find('a', class_='count_orange')
        if count_element:
            topic['replies'] = int(count_element.text.strip())
        else:
            topic['replies'] = 0
        
        # è·å–ç‚¹èµæ•°
        votes_element = item.find('div', class_='votes')
        if votes_element:
            votes_text = votes_element.get_text(strip=True)
            # æå–æ•°å­—
            votes_match = re.search(r'(\d+)', votes_text)
            if votes_match:
                topic['votes'] = int(votes_match.group(1))
            else:
                topic['votes'] = 0
        else:
            topic['votes'] = 0
        
        # è·å–ç²¾ç¡®å‘å¸ƒæ—¶é—´
        topic_info = item.find('span', class_='topic_info')
        if topic_info:
            # æŸ¥æ‰¾å¸¦ title å±æ€§çš„ spanï¼ˆåŒ…å«ç²¾ç¡®æ—¶é—´ï¼‰
            time_span = topic_info.find('span', title=True)
            if time_span:
                topic['created_time'] = time_span.get('title', '')
                topic['created_time_relative'] = time_span.get_text(strip=True)
            
            # è·å–æœ€åå›å¤è€…
            last_reply_text = topic_info.get_text()
            if 'æœ€åå›å¤æ¥è‡ª' in last_reply_text:
                last_reply_match = re.search(r'æœ€åå›å¤æ¥è‡ª.*?<strong><a[^>]*>([^<]+)</a>', str(topic_info))
                if last_reply_match:
                    topic['last_reply_user'] = last_reply_match.group(1)
        
        # è®°å½•æ”¶è—æ—¶é—´ï¼ˆå½“å‰æ—¶é—´ï¼‰
        topic['favorited_at'] = datetime.now().isoformat()
        
        return topic
        
    except Exception as e:
        print(f"âœ— è§£æä¸»é¢˜æ—¶å‡ºé”™: {e}")
        return None

def parse_favorites_page(html, current_page_num):
    """è§£ææ”¶è—é¡µé¢,æå–æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰æ”¶è—çš„ä¸»é¢˜
    items = soup.find_all('div', class_='cell item')
    
    topics = []
    for item in items:
        topic = parse_topic_from_item(item)
        if topic:
            topics.append(topic)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
    has_next = False
    all_links = soup.find_all('a')
    page_numbers = set()
    
    for link in all_links:
        href = link.get('href', '')
        if '/my/topics?p=' in href:
            try:
                page_num = int(href.split('p=')[1].split('&')[0].split('#')[0])
                if 1 <= page_num <= 1000:
                    page_numbers.add(page_num)
            except:
                pass
    
    if page_numbers and max(page_numbers) > current_page_num:
        has_next = True
    
    return topics, has_next

def remove_duplicates(topics):
    """æ ¹æ® topic ID å»é‡"""
    seen = set()
    unique_topics = []
    
    for topic in topics:
        topic_id = topic.get('id')
        if topic_id and topic_id not in seen:
            seen.add(topic_id)
            unique_topics.append(topic)
    
    return unique_topics

def export_to_markdown(topics, filename):
    """å¯¼å‡ºä¸º Markdown æ ¼å¼"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# V2EX æ”¶è—å¤‡ä»½\n\n")
        f.write(f"**å¤‡ä»½æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æ€»è®¡**: {len(topics)} ä¸ªä¸»é¢˜\n\n")
        
        # å†™å…¥æ‰€æœ‰ä¸»é¢˜
        f.write("## ğŸ“š æ‰€æœ‰æ”¶è—\n\n")
        
        # æŒ‰èŠ‚ç‚¹åˆ†ç»„
        topics_by_node = {}
        for topic in topics:
            node = topic.get('node', 'æœªåˆ†ç±»')
            if node not in topics_by_node:
                topics_by_node[node] = []
            topics_by_node[node].append(topic)
        
        for node, node_topics in sorted(topics_by_node.items()):
            f.write(f"### {node} ({len(node_topics)})\n\n")
            for topic in node_topics:
                f.write(f"- **[{topic['title']}]({topic['url']})**\n")
                f.write(f"  - ä½œè€…: [{topic.get('author', 'N/A')}]({topic.get('author_url', '#')})\n")
                f.write(f"  - å›å¤: {topic.get('replies', 0)} | ç‚¹èµ: {topic.get('votes', 0)}\n")
                if topic.get('created_time'):
                    f.write(f"  - å‘å¸ƒæ—¶é—´: {topic['created_time']}\n")
                f.write("\n")

def backup_all_favorites(output_dir=BACKUP_DIR):
    """å¤‡ä»½æ‰€æœ‰æ”¶è—çš„ä¸»é¢˜ - å¢å¼ºç‰ˆ"""
    print("\n" + "=" * 60)
    print("V2EX æ”¶è—å¤‡ä»½å·¥å…· - å¢å¼ºç‰ˆ")
    print("=" * 60)
    
    # åŠ è½½ Cookie
    cookie = load_cookie()
    if not cookie:
        return None
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    all_topics = []
    page = 1
    max_pages = 1000
    
    while page <= max_pages:
        print(f"\næ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
        
        html = get_favorites_page(cookie, page)
        if not html:
            print(f"âœ— æ— æ³•è·å–ç¬¬ {page} é¡µå†…å®¹")
            break
        
        # æ£€æŸ¥æ˜¯å¦ç™»å½•
        if 'ç™»å½•' in html and 'Google è´¦å·ç™»å½•' in html:
            print("\nâœ— Cookie å¯èƒ½å·²å¤±æ•ˆ,è¯·é‡æ–°è·å– Cookie!")
            return None
        
        topics, has_next = parse_favorites_page(html, page)
        
        if not topics:
            print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ”¶è—å†…å®¹,åœæ­¢è·å–")
            break
        
        all_topics.extend(topics)
        print(f"âœ“ ç¬¬ {page} é¡µ: è·å–åˆ° {len(topics)} ä¸ªæ”¶è— (ç´¯è®¡: {len(all_topics)})")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªä¸»é¢˜
        for i, topic in enumerate(topics[:3], 1):
            votes_info = f"ğŸ‘ {topic.get('votes', 0)}" if topic.get('votes', 0) > 0 else ""
            print(f"  {i}. {topic.get('title', 'N/A')} [{topic.get('replies', 0)} å›å¤] {votes_info}")
        
        if not has_next:
            print(f"\nâœ“ å·²åˆ°è¾¾æœ€åä¸€é¡µ (ç¬¬ {page} é¡µ)")
            break
        
        print(f"  â†’ æ£€æµ‹åˆ°æœ‰ä¸‹ä¸€é¡µ,ç»§ç»­...")
        page += 1
        time.sleep(1)
    
    if all_topics:
        # å»é‡
        original_count = len(all_topics)
        all_topics = remove_duplicates(all_topics)
        if original_count > len(all_topics):
            print(f"\nâœ“ å»é‡: ç§»é™¤äº† {original_count - len(all_topics)} ä¸ªé‡å¤é¡¹")
        
        # ä¿å­˜æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON æ ¼å¼
        json_filename = f"{output_dir}/favorites_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(all_topics, f, indent=2, ensure_ascii=False)
        
        # TXT æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        txt_filename = f"{output_dir}/favorites_{timestamp}.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"V2EX æ”¶è—å¤‡ä»½\n")
            f.write(f"å¤‡ä»½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è®¡: {len(all_topics)} ä¸ªä¸»é¢˜\n")
            f.write("=" * 60 + "\n\n")
            
            for i, topic in enumerate(all_topics, 1):
                f.write(f"{i}. {topic.get('title', 'N/A')}\n")
                f.write(f"   èŠ‚ç‚¹: {topic.get('node', 'N/A')} | ä½œè€…: {topic.get('author', 'N/A')}\n")
                f.write(f"   å›å¤: {topic.get('replies', 0)} | ç‚¹èµ: {topic.get('votes', 0)}\n")
                f.write(f"   é“¾æ¥: {topic.get('url', 'N/A')}\n")
                if topic.get('created_time'):
                    f.write(f"   å‘å¸ƒ: {topic['created_time']}\n")
                f.write("\n")
        
        # Markdown æ ¼å¼
        md_filename = f"{output_dir}/favorites_{timestamp}.md"
        export_to_markdown(all_topics, md_filename)
        
        print("\n" + "=" * 60)
        print("âœ“ å¤‡ä»½å®Œæˆ!")
        print(f"  æ€»å…±æ”¶è—: {len(all_topics)} ä¸ªä¸»é¢˜")
        print(f"\næ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“„ JSON: {json_filename}")
        print(f"  ğŸ“„ TXT:  {txt_filename}")
        print(f"  ğŸ“„ MD:   {md_filename}")
        print("=" * 60)
        
        return all_topics
    
    return None

def test_cookie(cookie):
    """æµ‹è¯• Cookie æ˜¯å¦æœ‰æ•ˆ"""
    print("æ­£åœ¨æµ‹è¯• Cookie...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Cookie": cookie,
    }
    
    try:
        response = requests.get(f"{BASE_URL}/my/topics", headers=headers, timeout=10)
        
        if response.status_code == 200:
            if 'ç™»å½•' in response.text and 'Google è´¦å·ç™»å½•' in response.text:
                print("âœ— Cookie æ— æ•ˆæˆ–å·²è¿‡æœŸ")
                return False
            else:
                print("âœ“ Cookie éªŒè¯æˆåŠŸ!")
                return True
        else:
            print(f"âœ— è¯·æ±‚å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âœ— æµ‹è¯•å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("V2EX æ”¶è—å¤‡ä»½å·¥å…·")
    print("=" * 60)
    
    # åŠ è½½ Cookie
    cookie = load_cookie()
    if not cookie:
        print("\nè·å– Cookie çš„æ­¥éª¤:")
        print("1. åœ¨æµè§ˆå™¨ä¸­ç™»å½• V2EX (https://v2ex.com)")
        print("2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("3. è¿›å…¥ Network(ç½‘ç»œ) æ ‡ç­¾")
        print("4. åˆ·æ–°é¡µé¢,ç‚¹å‡»ä»»æ„è¯·æ±‚")
        print("5. åœ¨ Headers ä¸­æ‰¾åˆ° 'Cookie' å­—æ®µå¹¶å¤åˆ¶å®Œæ•´å€¼")
        print(f"6. å°† Cookie å€¼ä¿å­˜åˆ° {COOKIE_FILE} æ–‡ä»¶ä¸­")
        exit(1)
    
    # æµ‹è¯• Cookie
    if not test_cookie(cookie):
        print("\nè¯·æ£€æŸ¥ä½ çš„ Cookie æ˜¯å¦æ­£ç¡®")
        exit(1)
    
    # å¼€å§‹å¤‡ä»½
    favorites = backup_all_favorites()
    
    if favorites:
        print(f"\næ”¶è—ä¸»é¢˜ç¤ºä¾‹ (å‰ 5 ä¸ª):\n")
        for i, topic in enumerate(favorites[:5], 1):
            print(f"{i}. {topic.get('title', 'N/A')}")
            print(f"   å›å¤: {topic.get('replies', 0)} | ç‚¹èµ: {topic.get('votes', 0)}")
            print(f"   {topic.get('url', 'N/A')}\n")
